import os
import pickle

import torch
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset, Subset
import random
import configparser
config = configparser.ConfigParser()
configFile = config.read('./pesidious.config')
dist = config['RL_train_massage']


seq_max_len = int(dist['max_seq_len'])
# seq_max_len=4096


def read_into_buffer(filename):
    buf = bytearray(os.path.getsize(filename))
    with open(filename, 'rb') as f:
        f.readinto(buf)
    f.close()
    return list(buf)


class MalwareDataset(Dataset):
    # def __init__(self, benign_dir="./Data/benign", malware_dir="./Mutated_malware/",malware_one_sample="[11,22,44]"):
    def __init__(self, benign_dir="./Data/benign",malware_one_sample="[11,22,44]"):
        # 恶性/良性程序文件夹
        self.benign_dir = benign_dir
        # self.malware_dir = malware_dir

        # 随机选取文件夹下一个良性文件
        self.benign_files = []
        self.benign_files_list = sorted(os.listdir(benign_dir))
        self.benign_files.append(random.choice(self.benign_files_list))
        #self.benign_files.append(random.choice(self.benign_files_list))

        # 选取当前待训练文件
        self.malware_files = []
        self.malware_files.append(malware_one_sample)

        # print(self.benign_files)
        # print(self.malware_files)
        # exit()

    # def printf(self):
    #     print("-1"*100)
    #     print(self.benign_files)
    #     print(self.malware_files)
    #     print("0"*100)

    # p=MalwareDataset，则p[0] = 第一个文件， p[1] = 第二个文件

    def __getitem__(self, index):
        try:
            # 先加载所有的benign数据集
            file_dir = os.path.join(self.benign_dir, self.benign_files[index])
            label = 0.0
        except IndexError:
            # 再加载所有的malware数据集
            file_dir = self.malware_files[index - len(self.benign_files)]
            label = 1.0

        if label==0.0:
            file_ = torch.tensor(read_into_buffer(file_dir))
            print("benign sample:")
            print(len(file_))
            print(file_)
            # exit()

        elif label==1.0:
            print("dataset")
            # print(type(file_dir))
            # print(file_dir)

            file_dir=file_dir.split(', ')
            file_dir=list(map(int, file_dir))
            # print(file_dir)
            # exit()
            file_ = torch.tensor(file_dir)
            print("malware sample:")
            print(len(file_))
            print(file_)
            # exit()
        # with open(file_dir, "rb") as f:

        # file_ = torch.tensor(pickle.load(f))

        return file_, label

    def __len__(self):
        return len(self.benign_files) + len(self.malware_files)


class UniLabelDataset(Dataset):
    def __init__(self, data_dir, is_malware):
        self.data_dir = data_dir
        self.is_malware = is_malware
        self.files = sorted(os.listdir(data_dir))

    def __getitem__(self, index):
        file_dir = os.path.join(self.data_dir, self.files[index])
        with open(file_dir, "rb") as f:
            file_ = torch.tensor(pickle.load(f))
        return file_, float(self.is_malware)

    def __len__(self):
        return len(self.files)


def collate_fn(batch):
    xs = pad_sequence([x[0] for x in batch], max_len=seq_max_len, padding_value=256)
    ys = torch.tensor([x[1] for x in batch])
    return xs, ys


def pad_sequence(sequences, max_len=None, padding_value=0):
    batch_size = len(sequences)
    if max_len is None:
        max_len = max([s.size(0) for s in sequences])
    out_tensor = sequences[0].new_full((batch_size, max_len), padding_value)
    for i, tensor in enumerate(sequences):
        length = tensor.size(0)
        if max_len > length:
            out_tensor[i, :length] = tensor
        else:
            out_tensor[i, :max_len] = tensor[:max_len]
    return out_tensor


def train_val_test_split(idx, val_size, test_size):
    # tv_idx, test_idx = train_test_split(idx, test_size=test_size, shuffle=True)
    # train_idx, val_idx = train_test_split(tv_idx, test_size=val_size, shuffle=True)
    tv_idx, test_idx = train_test_split(idx, test_size=test_size)
    train_idx, val_idx = train_test_split(tv_idx, test_size=val_size)
    return train_idx, val_idx, test_idx


def make_idx(dataset, val_size, test_size):
    num_benign = len(dataset.benign_files)
    num_malware = len(dataset.malware_files)
    benign_idx = range(num_benign)
    malware_idx = range(num_benign, num_benign + num_malware)
    benign_train_idx, benign_val_idx, benign_test_idx = train_val_test_split(
        benign_idx, val_size, test_size
    )
    malware_train_idx, malware_val_idx, malware_test_idx = train_val_test_split(
        malware_idx, val_size, test_size
    )
    train_idx = benign_train_idx + malware_train_idx
    val_idx = benign_val_idx + malware_val_idx
    test_idx = benign_test_idx + malware_test_idx
    return train_idx, val_idx, test_idx


def make_loaders(batch_size, val_size, test_size, benign_dir, malware_one_sample, seq_max_len1):
    global seq_max_len
    seq_max_len = seq_max_len1

    dataset = MalwareDataset(benign_dir=benign_dir, malware_one_sample=malware_one_sample)


    # dataset.printf()
    # train_idx, val_idx, test_idx = make_idx(dataset, val_size, test_size)
    train_idx=[0,1]
    # print("1"*100)
    # print(train_idx)

    train_dataset = Subset(dataset, indices=train_idx)
    # val_dataset = Subset(dataset, indices=val_idx)
    # test_dataset = Subset(dataset, indices=test_idx)
    # print("2"*100)
    # print(test_dataset)
    # print("-"*100)
    # exit()

    train_loader = make_loader(train_dataset, batch_size)
    # val_loader = make_loader(val_dataset, batch_size)
    # test_loader = make_loader(test_dataset, batch_size)
    # print(train_loader)
    # exit()
    return train_loader


def make_loader(dataset, batch_size):
    return DataLoader(
        dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True
    )



