import random
import gym
from collections import OrderedDict
from gym import error, spaces, utils
from gym.utils import seeding
import hashlib
import os
import torch
import numpy as np
from gym_malware.envs.utils import interface, pefeatures, pefeatures2

from gym_malware.envs.controls import manipulate2 as manipulate
ACTION_LOOKUP = {i: act for i, act in enumerate(
    manipulate.ACTION_TABLE.keys())}

import configparser
config = configparser.ConfigParser()
from pathlib import Path
import sys

module_path = os.path.dirname(os.path.abspath(sys.modules[__name__].__file__))
home = str(Path.home())
# home="E:\malware_rep\Pesidious-master - own"
home= os.path.join(module_path, '../../')
config_file = str(os.path.join(home, 'pesidious.config'))
configFile = config.read(config_file)
dist = config['RL_train_massage']
rl_mutations = int(dist['rl_mutations'])

# change this to function to the machine learning model you wish to attack
# function should be of the form
# def label_function( bytez ):
#    # returns score # number between 0.0 and 1.0, with benign=0.0 and malware=1.0
score_function = interface.get_score_local


# change this threshold to be the malicious/benign threshold
malicious_threshold = interface.local_model_threshold # if specified in params.json, then this is  interface.__private_data['threshold']


class MalwareScoreEnv(gym.Env):
    metadata = {'render.modes': ['human']}

    # def __init__(self, sha256list, random_sample=True, maxturns=80, output_path='evaded/score/', cache=False):
    def __init__(self, sha256list, random_sample=True, maxturns=rl_mutations, output_path='evaded/score/', cache=False):
        # print("jinru")
        # print(sha256list)
        # exit()
        self.cache = cache
        self.episode = 0
        self.available_sha256 = sha256list
        self.action_space = spaces.Discrete(len(ACTION_LOOKUP))
        self.maxturns = maxturns
        self.feature_extractor = pefeatures2.PEFeatureExtractor2()
        self.random_sample = random_sample
        self.sample_iteration_index = 0
        self.last_score = 0

        self.output_path = os.path.join(
            os.path.dirname(
                os.path.dirname(
                    os.path.dirname(
                        os.path.abspath(__file__)))), output_path)
        if not os.path.exists(output_path):
            os.makedirs(output_path)

        self.history = OrderedDict()

        self.samples = {}
        if self.cache:

            for sha256 in self.available_sha256:
                # print(sha256)
                try:
                    self.bytez = interface.fetch_file(self.sha256)

                except interface.FileRetrievalFailure:
                    print("failed fetching file")
                    continue  # try a new sha256...this one can't be retrieved from storage

        self.reset() # self.original_score, self.bytez and self.observation_space get set here
        # print("0")
        # exit()

    def step(self, action_index):

        # print("step:")
        # self.score = score_function(self.bytez)
        # print(self.score)
        # # print("1")
        # exit()
        self.turns += 1
        #print("episode : " + str(self + " turn : " + str(self.turns))

        self.take_action(action_index)
        # exit()
        # exit()
        # get reward
        try:
            # print("score*****:")
            # print(self.bytez[0:99])
            self.score = score_function(self.bytez)
            # print(self.score )
            # exit()
        except interface.ClassificationFailure:
            print("Failed to classify file")
            episode_over = True
        else:
            print("进入*******")
            # print()
            # exit()
            self.observation_space = self.feature_extractor.extract(self.bytez)

            print("malicious_threshold : " + str(malicious_threshold) + " ，score： " + str(self.score))
            # exit()
            if self.score < malicious_threshold:
                # we win!
                reward = 10.0
                episode_over = True
                self.history[self.sha256]['evaded'] = True

                # store sample to output directory
                m = hashlib.sha256()
                m.update( self.bytez )
                sha256 = m.hexdigest()
                # print("sha256")
                # print(sha256)
                # exit()
                self.history[self.sha256]['evaded_sha256'] = sha256      
                          
                with open( os.path.join( self.output_path, sha256), 'wb') as outfile:
                    outfile.write( self.bytez )

            #episode_over主要判断是否攻击成功
            elif self.turns >= self.maxturns:

                reward = self.original_score - self.score #

                episode_over = True

            else:
                reward = self.original_score - self.score # intermediate rewards
                # if reward == 0:
                #     reward = -0.1
                episode_over = False




        return np.asarray(self.observation_space), reward, episode_over, {}
        # return self.score, reward, episode_over, {}

    def take_action(self, action_index):
        assert action_index < len(ACTION_LOOKUP)
        # print(action_index)
        # print(ACTION_LOOKUP)
        # exit()
        action = ACTION_LOOKUP[action_index]

        #print("action : " + str(action))
        self.history[self.sha256]['actions'].append(action)

        self.bytez = bytes(
            manipulate.modify_without_breaking(self.bytez, [action]))
        # exit()





    def reset(self):

        #每次初始化时，使turns=0
        self.turns = 0
        #每次初始化时，使episode+1
        self.episode += 1


        #找到一个能使分类器判断为恶意样本的文件
        while True:
            # get the new environment
            if self.random_sample:
                self.sha256 = random.choice(self.available_sha256)
            else: # draw a sample at random
                self.sha256 = self.available_sha256[ self.sample_iteration_index % len(self.available_sha256) ]
                self.sample_iteration_index += 1

            print("current file")
            print(self.sha256)
            self.history[self.sha256] = {'actions': [], 'evaded': False}

            if self.cache:
                self.bytez = self.samples[self.sha256]
            else:
                try:
                    self.bytez = interface.fetch_file(self.sha256)
                except interface.FileRetrievalFailure:
                    print("failed fetching file")
                    continue  # try a new sha256...this one can't be retrieved from storage

            self.original_score = score_function(self.bytez)
            # print("original_score:")
            # print(self.original_score)
            # print(malicious_threshold)
            # exit()
            if self.original_score < malicious_threshold:
                # print(self.original_score)
                # exit()
                # skip this one, it's already benign, and the agent will learn nothing
                print("into")
                continue

            #print("new sha256: {}".format(self.sha256))

            self.observation_space = self.feature_extractor.extract(self.bytez)
            #print("original score: {}".format(self.original_score))

            break  # we're done here

        return np.asarray(self.observation_space)


    def render(self, mode='human', close=False):
        pass
