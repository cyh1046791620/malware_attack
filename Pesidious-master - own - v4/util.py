import torch
import random
import numpy as np
import os
from rich.panel import Panel
from rich.text import Text
from rich.table import Table
from pyfiglet import Figlet
import argparse
from pathlib import Path
from datetime import date
import logging
from rich.logging import RichHandler
from collections import namedtuple, deque
from logging import basicConfig, exception, debug, error, info, warning, getLogger
import configparser
config = configparser.ConfigParser()

configFile = config.read('./pesidious.config')
dist = config['RL_train_massage']
threshold = int(dist['threshold'])
device = dist['device']
if device == "cpu":
	device = torch.device("cpu")
elif device=="gpu":
	device = torch.device("cuda:0")

# ddqn prioritized replay buffer
class NaivePrioritizedBuffer(object):
	def __init__(self, capacity, prob_alpha=float(threshold/100)):
		self.prob_alpha = prob_alpha
		self.capacity   = capacity
		self.buffer     = []
		self.experience = namedtuple("Experience", field_names=["state", "action", "reward", "next_state", "done"])
		self.pos        = 0
		self.priorities = np.zeros((capacity,), dtype=np.float64)
		# self.priorities = np.zeros((capacity,), dtype=np.float64)

	def push(self, state, action, reward, next_state, done):

		max_prio = self.priorities.max() if self.buffer else 1.0

		if len(self.buffer) < self.capacity:
			e = self.experience(state, action, reward, next_state, done)
			self.buffer.append(e)
		else:
			e = self.experience(state, action, reward, next_state, done)
			self.buffer[self.pos] = e

		self.priorities[self.pos] = max_prio
		self.pos = (self.pos + 1) % self.capacity

	def sample(self, batch_size, beta=0.4):
		if len(self.buffer) == self.capacity:
			prios = self.priorities
			print("1")
			print("prios="+str(prios))
		else:
			prios = self.priorities[:self.pos]
			print("2")
			print("prios=" + str(prios))

		print("self.prob_alpha="+str(self.prob_alpha))
		probs  = prios ** self.prob_alpha

		#替换数组中的nan值为0
		a1 = np.isnan(probs)
		len_nan=0
		for index in range(len(probs)):
			if a1[index] == True:
				# probs[index] = 0
				len_nan += 1

		print("len_nan="+str(len_nan))
		print("probs.sum()="+str(probs.sum()))
		# print("type(probs.sum())="+str(type(probs.sum())))

		probs /= probs.sum()

		print("probs="+str(probs))
		# print("self.buffer=" + str(len(self.buffer)))
		# print("batch_size=" + str(batch_size))


		indices = np.random.choice(len(self.buffer), batch_size, p=probs)
		experiences = [self.buffer[idx] for idx in indices]

		# print([e.state.cpu() for e in experiences if e is not None])
		# np.vstack([e.state.cpu() for e in experiences if e is not None])

		states = torch.from_numpy(np.vstack([e.state.cpu() for e in experiences if e is not None])).float().to(device)
		actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)
		rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)
		next_states = torch.from_numpy(np.vstack([e.next_state.cpu() for e in experiences if e is not None])).float().to(device)
		dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)

		return (states, actions, rewards, next_states, dones, indices)

	def update_priorities(self, batch_indices, batch_priorities):
		for idx, prio in zip(batch_indices, batch_priorities):
			self.priorities[idx] = prio

	def __len__(self):
		return len(self.buffer)

#
def update_epsilon(n):
	epsilon_start = 1.0
	epsilon = epsilon_start
	epsilon_final = 0.4
	epsilon_decay = 1000 # N from the research paper (equation #6)

	epsilon = 1.0 - (n/epsilon_decay)

	if epsilon <= epsilon_final:
		epsilon = epsilon_final

	return epsilon

# create a dqn class





def put_banner():
	# Printing heading banner
	f = Figlet(font="banner4")
	grid = Table.grid(expand=True, padding=1, pad_edge=True)
	grid.add_column(justify="right", ratio=38)
	grid.add_column(justify="left", ratio=62)
	grid.add_row(
		Text.assemble((f.renderText("PE"), "bold red")),
		Text(f.renderText("Sidious"), "bold white"),
	)
	print(grid)
	print(
		Panel(
			Text.assemble(
				("Creating Chaos with Mutated Evasive Malware with ", "grey"),
				("Reinforcement Learning ", "bold red"),
				("and "),
				("Generative Adversarial Networks", "bold red"),
				justify="center",
			)
		)
	)

def parse_args():
	parser = argparse.ArgumentParser(description='Reinforcement Training Module')

	parser.add_argument('--rl_gamma', type=float, default=0.99, metavar='G',
						help='discount factor (default: 0.99)')
	parser.add_argument('--seed', type=int, default=42, metavar='N',
						help='random seed (default: 42)')

	parser.add_argument('--rl_episodes', type=float, default=1000,
						help='number of episodes to execute (default: 1000)')
	parser.add_argument('--rl_mutations', type=float, default=80,
						help='number of maximum mutations allowed (default: 80)')

	parser.add_argument('--rl_save_model_interval', type=float, default=500,
						help='Interval at which models should be saved (default: 500)') #gitul
	parser.add_argument('--rl_output_directory', type= Path, default=Path("models"),
						help='Path to save the models in (default: models)') #gitul

	parser.add_argument("--logfile", help = "The file path to store the logs. (default : rl_features_logs_" + str(date.today()) + ".log)", type = Path, default = Path("rl_features_logs_" + str(date.today()) + ".log"))
	logging_level = ["debug", "info", "warning", "error", "critical"]
	parser.add_argument(
		"-l",
		"--log",
		dest="log",
		metavar="LOGGING_LEVEL",
		choices=logging_level,
		default="info",
		help=f"Select the logging level. Keep in mind increasing verbosity might affect performance. Available choices include : {logging_level}",
	)

	args = parser.parse_args()
	return args

def logging_setup(logfile: str , log_level: str):

	from imp import reload
	reload(logging)

	log_dir = "Logs"

	if not os.path.exists(log_dir):
		os.mkdir(log_dir)

	logfile = os.path.join(log_dir, logfile)

	basicConfig(
		level=log_level.upper(),
		filemode='a',  # other options are w for write.
		format="%(message)s",
		filename=logfile
	)

	getLogger().addHandler(RichHandler())

	info("[*] Starting Reinforcement Learning Agent's Training ...\n")

def set_seed(seed):
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    #这里如果cuda报错就隐调
    torch.backends.cudnn.benchmark = True