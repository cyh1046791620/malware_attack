from classifier_code.train import train_last

import warnings
warnings.filterwarnings("ignore")

import logging
from logging import basicConfig, exception, debug, error, info, warning, getLogger
import argparse
from itertools import count

from pathlib import Path
from tqdm import tqdm
from datetime import date
import os

from rich.logging import RichHandler
from rich.progress import Progress, TaskID, track
from rich.traceback import install
from rich import print
from rich.panel import Panel
from rich.text import Text
from rich.table import Table
from pyfiglet import Figlet

from collections import namedtuple, deque
from statistics import mean

import math, random

import gym
import numpy as np
# np.random.seed(123)
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import gym_malware
from gym_malware.envs.utils import interface, pefeatures
from gym_malware.envs.controls import manipulate2 as manipulate
from collections import namedtuple, deque
from statistics import mean
import os
from strategy import get_strategy_class
import random
import numpy as np
from util import set_seed,put_banner,parse_args,logging_setup,NaivePrioritizedBuffer,update_epsilon

import configparser
config = configparser.ConfigParser()

configFile = config.read('./pesidious.config')
dist = config['RL_train_massage']

rl_gamma = float(dist['rl_gamma'])
seed = int(dist['seed'])
rl_episodes =  int(dist['rl_episodes'])
rl_mutations = int(dist['rl_mutations'])
rl_save_model_interval = int(dist['rl_save_model_interval'])
rl_output_directory = dist['rl_output_directory']
device = dist['device']
max_seq_len = int(dist['max_seq_len'])
replay_buffer_size = int(dist['replay_buffer_size'])
threshold = int(dist['threshold'])
malware_dir = dist['malware_dir']
benign_dir = dist['benign_dir']
saved_model = dist['saved_model']
strategy = dist['strategy']
pattern = dist['pattern']



put_banner()
args = parse_args()
logging_setup(str(args.logfile), args.log)

info("[*] Initilializing environment ...\n")
env_id = "malware-score-v0"
env = gym.make(env_id)

# env.seed(42)
# set_seed(42)
env.seed(seed)
set_seed(seed)

ACTION_LOOKUP = {i: act for i, act in enumerate(manipulate.ACTION_TABLE.keys())}

# device = torch.device("cpu")
# USE_CUDA = False
# Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)

if device == "cpu":
	device = torch.device("cpu")
elif device=="gpu":
	device = torch.device("cuda:0")


# class DQN(nn.Module):
# 	def __init__(self):
# 		super(DQN, self).__init__()
# 		self.layers = nn.Sequential(
# 			nn.Linear(env.observation_space.shape[0], 256),
# 			nn.ReLU(),
# 			nn.Linear(256, 64),
# 			nn.ReLU(),
# 			nn.Linear(64, env.action_space.n)
# 		)
#
# 	def forward(self, x):
# 		return self.layers(x)
#
#
# 	def chooseAction(self, observation, epsilon):
# 		# print("-" * 100)
# 		# print(epsilon)
# 		rand = np.random.random()
# 		# print(rand)
# 		# print("-" * 100)
# 		if rand > epsilon:
# 			#observation = torch.from_numpy(observation).float().unsqueeze(0).to(device)
# 			actions = self.forward(observation)
# 			action = torch.argmax(actions).item()
#
# 		else:
# 			action = np.random.choice(env.action_space.n)
#
# 		return action

#这里经验池太大，容易爆显存
# replay_buffer = NaivePrioritizedBuffer(500000)
# replay_buffer = NaivePrioritizedBuffer(2500)

replay_buffer = NaivePrioritizedBuffer(replay_buffer_size)

info("[*] Initilializing Neural Network model ...")

current_model = get_strategy_class(env.observation_space.shape[0],env.action_space.n,strategy)
target_model  = get_strategy_class(env.observation_space.shape[0],env.action_space.n,strategy)


optimizer = optim.Adam(current_model.parameters())
gamma = rl_gamma # discount factor as mentioned in the paper

def update_target(current_model, target_model):
	target_model.load_state_dict(current_model.state_dict())

# TD loss

def compute_td_loss(batch_size):
	state, action, reward, next_state, done, indices = replay_buffer.sample(batch_size, 0.4)


	Q_targets_next = target_model(next_state).detach().max(1)[0].unsqueeze(1)
	Q_targets = reward + (gamma * Q_targets_next * (1 - done))
	Q_expected = current_model(state).gather(1, action)

	loss  = (Q_expected - Q_targets.detach()).pow(2)
	prios = loss + 1e-5
	loss  = loss.mean()

	optimizer.zero_grad()
	loss.backward()
	replay_buffer.update_priorities(indices, prios.data.cpu().numpy())

	optimizer.step()

	return loss


#归一化特征
class RangeNormalize(object):
	def __init__(self,
				 min_val,
				 max_val):
		"""
		Normalize a tensor between a min and max value
		Arguments
		---------
		min_val : float
			lower bound of normalized tensor
		max_val : float
			upper bound of normalized tensor
		"""
		self.min_val = min_val
		self.max_val = max_val

	def __call__(self, *inputs):
		outputs = []
		for idx, _input in enumerate(inputs):
			_min_val = _input.min()
			_max_val = _input.max()
			a = (self.max_val - self.min_val) / (_max_val - _min_val)
			b = self.max_val- a * _max_val
			_input = (_input * a ) + b
			outputs.append(_input)
		return outputs if idx > 1 else outputs[0]

def main():
	info("[*] Starting training ...")

	#D为训练的最大次数
	#T为单轮最大修改次数
	# D = int(args.rl_episodes)
	D = rl_episodes

	T = rl_mutations

	#
	B = 1000 # as mentioned in the paper (number of steps before learning starts)

	batch_size = 32 # as mentioned in the paper (batch_size)


	losses = []
	reward_ben = 20

	#记录当前的训练步数
	n = 0 #current training step

	#创建rn归一化函数，归一区间为-0.5~0.5
	rn = RangeNormalize(-0.5,0.5)

	check = False

	detect_train_episode=0
	#进入训练,D是最大训练次数
	for episode in range(1, D):

		#用gym库的env创建第一个状态，这里注意的是每一个episode会随机选择一个恶意样本进行训练，并不是遍历所有恶意软件！
		#因此episode大概就等于训练过的恶意软件数量
		state = env.reset()
		print(type(state))
		# exit()


		#归一化
		# print(state)

		state_norm = rn(state)

		# print("state:")
		# print(state_norm)
		# exit()

		state_norm = torch.from_numpy(state_norm).float().unsqueeze(0).to(device)
		# print(state_norm)
		# exit()

		cur_state=[]
		done=False



		#在单轮中进行修改，最大为80次
		for mutation in range(1, T):

			#跟新时间步
			n = n + 1

			epsilon = update_epsilon(n)

			#选择一个动作空间中的一个动作（根据状态与epsilon）
			action = current_model.chooseAction(state_norm, epsilon)


			# 用gym库获取当前时间步执行该action的得分，以及得到下一个状态
			#next_state==下一个状态
			#reward=获得
			#done=是否修改成功
			print("")
			# exit()
			next_state, reward, done, _ = env.step(action)
			cur_state=next_state



			# exit()
			# if mutation == 12:
			# 	exit()
			# print(5)
			# next_state=next_state.numpy().tolist()
			# print(next_state)
			# info("5")

			##更新代码##

			# if reward==0:
			# 	reward=-10

			##更新代码##

			# print('reward:')
			# print(reward)
			# exit()

			#输出该Episode第mutation次修改的得分
			print("\t[+] Episode : " + str(episode) + " , Mutation # : " + str(mutation) + " , Mutation : " + str(ACTION_LOOKUP[action]) + " , Reward : " + str(reward))

			# print(next_state)
			#下一个状态的归一化操作
			next_state_norm = rn(next_state)
			#
			# print(len(next_state_norm))
			# exit()
			#
			next_state_norm = torch.from_numpy(next_state_norm).float().unsqueeze(0).to(device)
			# print(next_state_norm)
			# exit()

			if reward == 10.0:
				power = -((mutation-1)/T)
				reward = (math.pow(reward_ben, power))*100


			#回放池更新，用于记录
			replay_buffer.push(state_norm, action, reward, next_state_norm, done)

			print("Len_buffer="+str(len(replay_buffer.buffer)))
			# print(replay_buffer.buffer)
			# exit()




			if len(replay_buffer) > B:
				loss = compute_td_loss(batch_size)
				losses.append(loss.item())




			#如果完成攻击，则跳出循环
			if done:
				break

			state_norm = next_state_norm

		# 检测代理部分
		#如果完成攻击，并且模式是双代理模式，则开启检测代理的过程
		if done and pattern=="Double_agent":
			detect_train_episode=detect_train_episode+1
			print("-"*100)
			print("Detection train episode = "+str(detect_train_episode))
			print("start detect train:")
			list_1=cur_state.tolist()

			train_last(str(list_1).lstrip('[').rstrip(']'))


		#每轮结束清理一下缓存。
		torch.cuda.empty_cache()
		# replay_buffer.buffer = []

		# print()
		# exit()
		debug('\t[+] Episode Over')

		if n % 100 == 0:
			#将当前model更新到target_model中（ddqn）
			update_target(current_model, target_model)

			#每隔rl_save_model_interval保存model
		if episode % args.rl_save_model_interval == 0:
			if not os.path.exists(rl_output_directory):
				os.mkdir(rl_output_directory)
				info("[*] model directory has been created at : " + str(rl_output_directory))
			torch.save(current_model.state_dict(), os.path.join(rl_output_directory,
																"rl-model-" + str(episode) + "-" + str(
																	date.today()) + ".pt"))
			info("[*] Saving model in models/ directory ...")
		# if episode % args.rl_save_model_interval == 0:
		# 	if not os.path.exists(args.rl_output_directory):
		# 		os.mkdir(args.rl_output_directory)
		# 		info("[*] model directory has been created at : " + str(args.rl_output_directory))
		# 	torch.save(current_model.state_dict(), os.path.join(args.rl_output_directory, "rl-model-" + str(episode) + "-" +str(date.today()) + ".pt" ))
		# 	info("[*] Saving model in models/ directory ...")


	torch.save(current_model.state_dict(), os.path.join(args.rl_output_directory, "rl-model-" + str(D) + "-" +str(date.today()) + ".pt" ))
	info("[*] Saving model in models/ directory ...")



if __name__ == '__main__':

	main()
























